як реалізувати виведення mAP метрики у цій моделі
прикріпляю model.py, head.py, encoder.py, train.py, dataloader.py

```
import numpy as np


def gaussian_radius(det_size, min_overlap=0.7):
    height, width = det_size

    a1 = 1
    b1 = height + width
    c1 = width * height * (1 - min_overlap) / (1 + min_overlap)
    sq1 = np.sqrt(b1**2 - 4 * a1 * c1)
    r1 = (b1 + sq1) / 2

    a2 = 4
    b2 = 2 * (height + width)
    c2 = (1 - min_overlap) * width * height
    sq2 = np.sqrt(b2**2 - 4 * a2 * c2)
    r2 = (b2 + sq2) / 2

    a3 = 4 * min_overlap
    b3 = -2 * min_overlap * (height + width)
    c3 = (min_overlap - 1) * width * height
    sq3 = np.sqrt(b3**2 - 4 * a3 * c3)
    r3 = (b3 + sq3) / 2
    return min(r1, r2, r3)


def gaussian2D(shape, sigma_x=1, sigma_y=1):
    m, n = [(ss - 1.0) / 2.0 for ss in shape]
    y, x = np.ogrid[-m : m + 1, -n : n + 1]
    h = np.exp(-x * x / (2 * sigma_x**2) - y * y / (2 * sigma_y**2))
    h[h < np.finfo(h.dtype).eps * h.max()] = 0
    return h


def draw_gaussian(heatmap, center, radius_x, radius_y):
    diameter_x = 2 * radius_x + 1
    diameter_y = 2 * radius_y + 1
    gaussian = gaussian2D(
        (diameter_y, diameter_x), sigma_y=diameter_y / 6, sigma_x=diameter_x / 6
    )

    x, y = int(np.round(center[0])), int(np.round(center[1]))

    height, width = heatmap.shape[0:2]

    left, right = min(x, radius_x), min(width - x, radius_x + 1)
    top, bottom = min(y, radius_y), min(height - y, radius_y + 1)

    masked_heatmap = heatmap[y - top : y + bottom, x - left : x + right]
    masked_gaussian = gaussian[
        radius_y - top : radius_y + bottom, radius_x - left : radius_x + right
    ]
    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:
        np.maximum(masked_heatmap, masked_gaussian, out=masked_heatmap)
    return heatmap


class CenternetEncoder:
    def __init__(self, img_height=320, img_width=320, down_ratio=4, n_classes=20):
        self._img_height = img_height
        self._img_width = img_width
        self._out_height = self._img_height // down_ratio
        self._out_width = self._img_width // down_ratio
        self._n_classes = n_classes
        self._down_ratio = down_ratio
        print("down_ratio = {}".format(self._down_ratio))

    def __call__(self, bboxes, labels):

        hm = np.zeros(
            (self._out_height, self._out_width, self._n_classes), dtype=np.float32
        )
        coors = np.zeros((self._out_height, self._out_width, 4), dtype=np.float32)
        for cls_id, bbox in zip(labels.data.numpy(), bboxes.data.numpy()):
            box_s = bbox / self._down_ratio
            h, w = box_s[3] - box_s[1], box_s[2] - box_s[0]
            rad_w_class = int(np.round(gaussian_radius([h, w])))
            rad_h_class = rad_w_class
            if h > 0 and w > 0:
                center = np.array(
                    [(box_s[0] + box_s[2]) / 2, (box_s[1] + box_s[3]) / 2],
                    dtype=np.float32,
                )
                center = np.round(center)
                center = np.clip(
                    center, [0, 0], [self._out_width - 1, self._out_height - 1]
                )
                center_int = center.astype(np.int32)
                draw_gaussian(hm[..., cls_id - 1], center_int, rad_w_class, rad_h_class)
                coors[center_int[1], center_int[0]] = bbox
        return np.concatenate((hm, coors), axis=-1)
```


```
from collections import OrderedDict

import torch
import torch.nn as nn
import torch.nn.functional as F


class Head(nn.Module):
    def __init__(self, backbone_output_filters, class_number=20):
        super().__init__()
        self.connection_num = 3
        self.class_number = class_number
        self.backbone_output_filters = backbone_output_filters
        self.filters = [128, 64, 32]
        head_filters = [self.backbone_output_filters[-1]] + self.filters

        for i, filter_num in enumerate(self.filters):
            name = f"head_{i+1}"
            setattr(
                self,
                name,
                self.conv_bn_relu(name, head_filters[i], head_filters[i + 1]),
            )
            # create connection with backbone
            if i < self.connection_num:
                name = f"after_{-2-i}"
                setattr(
                    self,
                    name,
                    self.conv_bn_relu(
                        name, self.backbone_output_filters[-2 - i], self.filters[i], 1
                    ),
                )

        self.before_hm = self.conv_bn_relu(
            "before_hm", self.filters[-1], self.filters[-1]
        )
        self.before_sizes = self.conv_bn_relu(
            "before_sizes", self.filters[-1], self.filters[-1]
        )

        self.hm = self.conv_bn_relu(
            "hm", self.filters[-1], self.class_number, 3, "sigmoid"
        )
        self.sizes = self.conv_bn_relu("hm", self.filters[-1], 4, 3, None)

    def conv_bn_relu(
        self, name, input_num, output_num, kernel_size=3, activation="relu"
    ):
        block = OrderedDict()
        padding = 1 if kernel_size == 3 else 0
        block["conv_" + name] = nn.Conv2d(
            input_num, output_num, kernel_size=kernel_size, stride=1, padding=padding
        )
        block["bn_" + name] = nn.BatchNorm2d(output_num, eps=1e-3, momentum=0.01)
        if activation == "relu":
            block["relu_" + name] = nn.ReLU()
        elif activation == "sigmoid":
            block["sigmoid_" + name] = nn.Sigmoid()
        return nn.Sequential(block)

    def connect_with_backbone(self, *backbone_out):
        used_out = [backbone_out[-i - 2] for i in range(self.connection_num)]
        x = backbone_out[-1]
        for i in range(len(self.filters)):
            x = getattr(self, "head_{}".format(i + 1))(x)
            x = F.interpolate(x, scale_factor=2, mode="nearest")
            if i < self.connection_num:
                name = f"after_{-2-i}"
                x_ = getattr(self, name)(used_out[i])
                x = torch.add(x, x_)
        return x

    def forward(self, *backbone_out):
        self.last_shared_layer = self.connect_with_backbone(self, *backbone_out)
        x = self.before_hm(self.last_shared_layer)
        hm_out = self.hm(x)

        x = self.before_sizes(self.last_shared_layer)
        sizes_out = self.sizes(x)

        x = torch.cat((hm_out, sizes_out), dim=1)
        return x
```


```
import torch.nn as nn

from losses.centernet_ttf import CenternetTTFLoss
from backbones import create_backbone
from heads.centernet_head import Head
from utils.config import IMG_HEIGHT, IMG_WIDTH

class ModelBuilder(nn.Module):
    """
    To connect head with backbone
    """

    def __init__(
        self,
        alpha=1.0,
        class_number=20,
        backbone: str = "default",
        backbone_weights: str = None,
    ):
        super().__init__()
        self.class_number = class_number
        self.backbone = create_backbone(backbone, alpha, backbone_weights)
        self.head = Head(
            backbone_output_filters=self.backbone.filters, class_number=class_number
        )
        self.loss = CenternetTTFLoss(

            class_number,
            4,
            IMG_HEIGHT // 4,
            IMG_WIDTH // 4,
        )


    def forward(self, x, gt=None):
        x = x / 0.5 - 1.0  # normalization
        out = self.backbone(x)
        pred = self.head(*out)

        if gt is None:
            return pred
        else:
            loss = self.loss(gt, pred)
            return loss
```