{
 "cells": [
  {
   "cell_type": "code",
   "id": "db14077e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db14077e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1721401307695,
     "user_tz": -180,
     "elapsed": 10406,
     "user": {
      "displayName": "Vitaliy Bulygin",
      "userId": "12051646707039759689"
     }
    },
    "outputId": "6e1b8c86-9a66-4b1c-f3af-79043d979747"
   },
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torch\n",
    "from training.encoder import CenternetEncoder\n",
    "from utils.tmp_visualizer import get_image_with_bboxes\n",
    "from models.centernet import ModelBuilder\n",
    "from data.dataset import Dataset\n",
    "\n",
    "input_height, input_width = 256, 256\n",
    "\n",
    "print(\"GPU is available: \", torch.cuda.is_available())\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12.0, 8.0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2107803b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2107803b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1721401310263,
     "user_tz": -180,
     "elapsed": 2570,
     "user": {
      "displayName": "Vitaliy Bulygin",
      "userId": "12051646707039759689"
     }
    },
    "outputId": "83dfd4f9-c621-4d30-8eb4-89acc6818b36"
   },
   "source": [
    "dataset_val = torchvision.datasets.VOCDetection(\n",
    "    root=\"../VOC\", year=\"2007\", image_set=\"val\", download=False\n",
    ")\n",
    "dataset_val = torchvision.datasets.wrap_dataset_for_transforms_v2(dataset_val)\n",
    "\n",
    "# these 10 pictures from the VOC dataset were randomly selected for model training\n",
    "trainingdata_indices = torch.tensor(\n",
    "    [955, 1025, 219, 66, 1344, 222, 865, 2317, 86, 1409]\n",
    ")\n",
    "\n",
    "print(len(dataset_val))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ModelBuilder(alpha=0.25, filters_size=[128, 64, 32]).to(device)\n",
    "model.load_state_dict(\n",
    "    torch.load(\"../models/checkpoints/tmp_trained_model.pt\", weights_only=True)\n",
    ")"
   ],
   "id": "c7b77ae97fd9f58b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# just to explore heatmap for some specific image used in model training\n",
    "\n",
    "image_index = 5  # 0 (car) is also a good choice\n",
    "\n",
    "img, lbl = dataset_val[trainingdata_indices[image_index]]\n",
    "print(lbl)\n",
    "\n",
    "#\n",
    "image_with_boxes = get_image_with_bboxes(img, lbl[\"boxes\"], lbl[\"labels\"])\n",
    "plt.imshow(image_with_boxes)"
   ],
   "id": "2862d9dc559c53f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "transform_resize = transforms.Compose(\n",
    "    [transforms.Resize(size=(input_width, input_height))]\n",
    ")\n",
    "img_transformed, bboxes, labels = transform_resize(img, lbl[\"boxes\"], lbl[\"labels\"])\n",
    "\n",
    "image_with_boxes = get_image_with_bboxes(img_transformed, bboxes, labels)\n",
    "plt.imshow(image_with_boxes)"
   ],
   "id": "b9a93732f15c5163",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "encoder = CenternetEncoder(input_height, input_width)\n",
    "lbl_encoded = encoder(bboxes, labels)\n",
    "\n",
    "for i in range(20):\n",
    "    hm_chosen_current = lbl_encoded[..., i]\n",
    "    print(f\"i = {i + 1}; np.amax(hm_chosen) = {np.amax(hm_chosen_current)}\")\n",
    "print()"
   ],
   "id": "df26c6e6d373cafb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "i = lbl[\"labels\"][0]  # corresponds to 'person'\n",
    "hm_chosen = lbl_encoded[..., i - 1]\n",
    "print(f\"i = {i}; np.amax(hm_chosen) = {np.amax(hm_chosen)}\")\n",
    "print()\n",
    "\n",
    "ind_max = np.argwhere(hm_chosen == np.amax(hm_chosen))\n",
    "for ind in ind_max:\n",
    "    print(\"rect center:\", ind * 4)\n",
    "    print(\"coors\", lbl_encoded[..., 20:][ind[0], ind[1]])\n",
    "    print()\n",
    "\n",
    "plt.imshow(lbl_encoded[..., lbl[\"labels\"][0] - 1])"
   ],
   "id": "5341e929c4fde269",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)\n",
    "\n",
    "for y in range(5):\n",
    "    for i in range(4):\n",
    "        plt_idx = i + y * 4 + 1\n",
    "        plt.subplot(4, 5, plt_idx)\n",
    "        plt.imshow(lbl_encoded[..., plt_idx - 1])\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "id": "8f5da8ea312b4397",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter the entire VOC dataset to get 12 images containing persons. Label = 15\n",
    "number_of_persons = 12\n",
    "person_label = 15\n",
    "\n",
    "person_images = []\n",
    "testdata_indexes = []\n",
    "\n",
    "for index, (img, lbl) in enumerate(dataset_val):\n",
    "    if person_label in lbl[\"labels\"]:\n",
    "        testdata_indexes.append(index)\n",
    "        person_images.append({\"index\": index, \"image\": img, \"lbl\": lbl})\n",
    "    if len(testdata_indexes) == number_of_persons:\n",
    "        break\n",
    "\n",
    "print(f\"There are {len(person_images)} persons in dataset\")\n",
    "\n",
    "# Visualize first 10 persons from the dataset.\n",
    "# They form my test data.\n",
    "for y in range(4):\n",
    "    for i in range(3):\n",
    "        plt_idx = i + y * 3 + 1\n",
    "        plt.subplot(3, 4, plt_idx)\n",
    "\n",
    "        img_transformed, bboxes, labels = transform_resize(\n",
    "            person_images[plt_idx - 1][\"image\"],\n",
    "            person_images[plt_idx - 1][\"lbl\"][\"boxes\"],\n",
    "            person_images[plt_idx - 1][\"lbl\"][\"labels\"],\n",
    "        )\n",
    "\n",
    "        image_with_boxes = get_image_with_bboxes(img_transformed, bboxes, labels)\n",
    "        plt.imshow(image_with_boxes)\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "id": "76ce27e5eb0906b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "\n",
    "# I cannot get predictions without train(True)\n",
    "model.train(True)"
   ],
   "id": "9459a8521fd71e2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(input_width, input_height)),\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")\n",
    "torch_dataset = Dataset(dataset=dataset_val, transformation=transform, encoder=encoder)\n",
    "\n",
    "training_data = torch.utils.data.Subset(torch_dataset, trainingdata_indices)\n",
    "test_data = torch.utils.data.Subset(torch_dataset, testdata_indexes)\n",
    "\n",
    "# comment this line when you need calculations on really test data\n",
    "test_data = training_data\n",
    "\n",
    "# Here I am getting the loss for test data received with trained model\n",
    "\n",
    "# todo (AA): I don't completely understand what this prediction contain\n",
    "batch_generator = torch.utils.data.DataLoader(test_data, num_workers=4, batch_size=12)\n",
    "for input_data, gt_data in batch_generator:\n",
    "    input_contiguous = input_data.to(device).contiguous()\n",
    "    gt_data_device = gt_data.to(device)\n",
    "    # result = model.forward(img_reshaped.to(device), gt=gt_data_device)\n",
    "\n",
    "    result = model.forward(input_contiguous, gt=gt_data_device)\n",
    "    print(result[\"loss\"])\n",
    "\n",
    "    # it gives the same result as above\n",
    "    # result = model(input_contiguous)\n",
    "    # print(model.loss(gt_data_device, result)['loss'])"
   ],
   "id": "eeba37d24d0e9c51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "aa7386f7fd221436",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
